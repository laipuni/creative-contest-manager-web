{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wPgd0Tl60QTX"},"outputs":[],"source":["# 1. Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å ÎßàÏö¥Ìä∏\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLpSHztm1Qwk"},"outputs":[],"source":["# 2. ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò\n","!pip install fastapi pyngrok uvicorn fsspec==2025.3.2 transformers datasets accelerate nest_asyncio --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMAxICaQvIb7"},"outputs":[],"source":["# 3. Ï£ºÏöî ÎùºÏù¥Î∏åÎü¨Î¶¨ Î°úÎî©\n","from fastapi import FastAPI\n","from pydantic import BaseModel\n","from pyngrok import ngrok\n","import nest_asyncio\n","import uvicorn\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerFast, GPT2LMHeadModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zz6OeeasAn-Z"},"outputs":[],"source":["# 4. Math Î¨∏Ï†ú ÏÉùÏÑ± Î™®Îç∏ Î°úÎî©\n","math_problem_model_path = \"/content/drive/MyDrive/Colab Notebooks/KoGPT/Model/Math/Problem\"\n","math_problem_tokenizer = AutoTokenizer.from_pretrained(math_problem_model_path)\n","math_problem_model = AutoModelForCausalLM.from_pretrained(math_problem_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["# 5. Math Ï†ïÎãµ ÏÉùÏÑ± Î™®Îç∏ Î°úÎî©\n","math_answer_model_path = \"/content/drive/MyDrive/Colab Notebooks/KoGPT/Model/Math/Answer\"\n","math_answer_tokenizer = AutoTokenizer.from_pretrained(math_answer_model_path)\n","math_answer_model = AutoModelForCausalLM.from_pretrained(math_answer_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"qi-7DwFVY6Xm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RF6kHNHUFStb"},"outputs":[],"source":["# 6. Logic Î¨∏Ï†ú ÏÉùÏÑ± Î™®Îç∏ Î°úÎî©\n","logic_problem_model_path = \"/content/drive/MyDrive/Colab Notebooks/KoGPT/Model/Logic/Problem\"\n","logic_problem_tokenizer = AutoTokenizer.from_pretrained(logic_problem_model_path)\n","logic_problem_model = AutoModelForCausalLM.from_pretrained(logic_problem_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["# 7. Logic Ï†ïÎãµ ÏÉùÏÑ± Î™®Îç∏ Î°úÎî©\n","logic_answer_model_path = \"/content/drive/MyDrive/Colab Notebooks/KoGPT/Model/Logic/Answer\"\n","logic_answer_tokenizer = AutoTokenizer.from_pretrained(logic_answer_model_path)\n","logic_answer_model = AutoModelForCausalLM.from_pretrained(logic_answer_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"rDYPcoZrN3Wj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkarY0ECFR8a"},"outputs":[],"source":["# 8. Knowledge Î¨∏Ï†ú ÏÉùÏÑ± Î™®Îç∏ Î°úÎî©\n","knowledge_problem_model_path = \"/content/drive/MyDrive/Colab Notebooks/KoGPT/Model/Knowledge/Problem\"\n","knowledge_problem_tokenizer = AutoTokenizer.from_pretrained(knowledge_problem_model_path)\n","knowledge_problem_model = AutoModelForCausalLM.from_pretrained(knowledge_problem_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["# 9. Knowledge Ï†ïÎãµ ÏÉùÏÑ± Î™®Îç∏ Î°úÎî©\n","knowledge_answer_model_path = \"/content/drive/MyDrive/Colab Notebooks/KoGPT/Model/Knowledge/Answer\"\n","knowledge_answer_tokenizer = AutoTokenizer.from_pretrained(knowledge_answer_model_path)\n","knowledge_answer_model = AutoModelForCausalLM.from_pretrained(knowledge_answer_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"PM5p8RHjS6q5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XT1eJwSqFSV8"},"outputs":[],"source":["# 10. Open Problem Î¨∏Ï†ú ÏÉùÏÑ± Î™®Îç∏ Î°úÎî©\n","open_problem_model_path = \"/content/drive/MyDrive/Colab Notebooks/KoGPT/Model/Open/Problem\"\n","open_problem_tokenizer = AutoTokenizer.from_pretrained(open_problem_model_path)\n","open_problem_model = AutoModelForCausalLM.from_pretrained(open_problem_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["# 11. Î¨∏Ï†ú ÏÉùÏÑ± Ìï®Ïàò\n","def problem(prompt, model, tokenizer):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"],\n","        max_new_tokens=100,\n","        temperature=0.8,\n","        top_p=0.9,\n","        pad_token_id=tokenizer.eos_token_id,\n","        do_sample=True,\n","        repetition_penalty=1.1,\n","        eos_token_id=tokenizer.convert_tokens_to_ids(\"<END>\")\n","    )\n","\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    print(\"[Î¨∏Ï†úÏÉùÏÑ±Í≤∞Í≥º]\\n\", generated_text)\n","    return generated_text.split(\"Î¨∏Ï†ú: \")[1].strip()"],"metadata":{"id":"LjSDn0T-ftGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 12. Ï†ïÎãµ ÏÉùÏÑ± Ìï®Ïàò\n","def answer(prompt, model, tokenizer):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"],\n","        max_new_tokens=100,\n","        temperature=0.8,\n","        top_p=0.8,\n","        pad_token_id=tokenizer.pad_token_id,\n","        do_sample=True,\n","        repetition_penalty=1.1,\n","        eos_token_id=tokenizer.convert_tokens_to_ids(\"<END>\")\n","    )\n","\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    print(\"[Ï†ïÎãµÏÉùÏÑ±Í≤∞Í≥º]\\n\", generated_text)\n","    print()\n","    return generated_text.split(\"Ï†ïÎãµ: \")[1].strip()"],"metadata":{"id":"s3DGySBqfs2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xDJi7P2qFlC"},"outputs":[],"source":["# 13. FastAPI app ÏÑ§Ï†ï\n","app = FastAPI()\n","\n","class ProblemRequest(BaseModel):\n","    topic: str\n","    level: str\n","    count: int\n","\n","class ProblemResponse(BaseModel):\n","    response: list[str]\n","\n","@app.post(\"/generate\", response_model=ProblemResponse)\n","def generate_problems(req: ProblemRequest):\n","    global math_problem_model, math_problem_tokenizer, math_answer_model, math_answer_tokenizer\n","    global logic_problem_model, logic_problem_tokenizer, logic_answer_model, logic_answer_tokenizer\n","    global knowledge_problem_model, knowledge_problem_tokenizer, knowledge_answer_model, knowledge_answer_tokenizer\n","    global open_problem_model, open_problem_tokenizer\n","\n","    topic = req.topic\n","    problem_list = []\n","\n","    if topic == 'ÏàòÌïô':\n","      problem_model = math_problem_model\n","      problem_tokenizer = math_problem_tokenizer\n","      answer_model = math_answer_model\n","      answer_tokenizer = math_answer_tokenizer\n","    elif topic == 'ÎÖºÎ¶¨':\n","      problem_model = logic_problem_model\n","      problem_tokenizer = logic_problem_tokenizer\n","      answer_model = logic_answer_model\n","      answer_tokenizer = logic_answer_tokenizer\n","    elif topic == 'ÏÉÅÏãù':\n","      problem_model = knowledge_problem_model\n","      problem_tokenizer = knowledge_problem_tokenizer\n","      answer_model = knowledge_answer_model\n","      answer_tokenizer = knowledge_answer_tokenizer\n","    elif topic == 'Ïó¥Î¶∞Î¨∏Ï†ú':\n","      problem_model = open_problem_model\n","      problem_tokenizer = open_problem_tokenizer\n","    else:\n","      return ProblemResponse(response=problem_list)\n","\n","    for _ in range(req.count):\n","        # problem_prompt = f\"ÎÇúÏù¥ÎèÑ: {req.level}\\nÎ¨∏Ï†ú: \"\n","        problem_prompt = f\"Î¨∏Ï†ú: \"\n","        problem_result = problem(problem_prompt, problem_model, problem_tokenizer)\n","\n","        if topic == 'Ïó¥Î¶∞Î¨∏Ï†ú':\n","          answer_result = 'ÏóÜÏùå'\n","        else:\n","          answer_prompt = f\"Î¨∏Ï†ú: {problem_result}\\nÏ†ïÎãµ: \"\n","          answer_result = answer(answer_prompt, answer_model, answer_tokenizer)\n","\n","        temp = f\"Î¨∏Ï†ú: {problem_result}\\nÏ†ïÎãµ: {answer_result}\"\n","        problem_list.append(temp)\n","\n","    return ProblemResponse(response=problem_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"345RPuj4jYpU"},"outputs":[],"source":["# 14. Ngrok Ïã§Ìñâ\n","ngrok.set_auth_token(\"***\")\n","ngrok.kill()\n","\n","public_url = ngrok.connect(3000)\n","print(\"üîó Public URL:\", public_url.public_url)\n","\n","nest_asyncio.apply()\n","uvicorn.run(app, host=\"0.0.0.0\", port=3000)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1h1WVOwUWaOeMV3nEbSg-opXZej90aLPC","timestamp":1745922267590}],"mount_file_id":"1h1WVOwUWaOeMV3nEbSg-opXZej90aLPC","authorship_tag":"ABX9TyOzbvumzsBYPJP9F+rz+fUa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}