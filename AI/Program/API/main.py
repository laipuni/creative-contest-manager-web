# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ttHRzFNai6PMBRPiC01w3Mkk5o5vrU1E
"""

# ---------------------------------------------------------------------
### CP code
# ---------------------------------------------------------------------

# 3. 주요 라이브러리 로딩
from fastapi import FastAPI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerFast, GPT2LMHeadModel
import numpy as np
import nest_asyncio
import uvicorn
import faiss
import torch
import os

from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

def get_model_path(*path_parts):
    base = os.environ.get("MODEL_BASE_PATH", "/app/models")  # fallback
    # Path 객체로 반환 (절대 경로 변환 포함)
    return Path(base).joinpath(*path_parts).resolve()


# 4. BERT 유사도 검색
embedding_model = SentenceTransformer('jhgan/ko-sbert-sts')
embedding_dim = embedding_model.get_sentence_embedding_dimension()

# 5. Math 문제 생성 모델 로딩
math_problem_model_path = get_model_path("Math", "Problem")
math_problem_tokenizer = AutoTokenizer.from_pretrained(math_problem_model_path,local_files_only=True)
math_problem_model = AutoModelForCausalLM.from_pretrained(math_problem_model_path,local_files_only=True).to("cuda" if torch.cuda.is_available() else "cpu")

# 6. Math 정답 생성 모델 로딩
math_answer_model_path = get_model_path("Math","Answer")
math_answer_tokenizer = AutoTokenizer.from_pretrained(math_answer_model_path,local_files_only=True)
math_answer_model = AutoModelForCausalLM.from_pretrained(math_answer_model_path,local_files_only=True).to("cuda" if torch.cuda.is_available() else "cpu")

# 7. Logic 문제 생성 모델 로딩
logic_problem_model_path =  get_model_path("Logic","Problem")
logic_problem_tokenizer = AutoTokenizer.from_pretrained(logic_problem_model_path,local_files_only=True)
logic_problem_model = AutoModelForCausalLM.from_pretrained(logic_problem_model_path,local_files_only=True).to("cuda" if torch.cuda.is_available() else "cpu")

# 8. Logic 정답 생성 모델 로딩
logic_answer_model_path = get_model_path("Logic","Answer")
logic_answer_tokenizer = AutoTokenizer.from_pretrained(logic_answer_model_path,local_files_only=True)
logic_answer_model = AutoModelForCausalLM.from_pretrained(logic_answer_model_path,local_files_only=True).to("cuda" if torch.cuda.is_available() else "cpu")

# 9. Knowledge 문제 생성 모델 로딩
knowledge_problem_model_path = get_model_path("Knowledge","Problem")
knowledge_problem_tokenizer = AutoTokenizer.from_pretrained(knowledge_problem_model_path,local_files_only=True)
knowledge_problem_model = AutoModelForCausalLM.from_pretrained(knowledge_problem_model_path,local_files_only=True).to("cuda" if torch.cuda.is_available() else "cpu")

# 10. Knowledge 정답 생성 모델 로딩
knowledge_answer_model_path = get_model_path("Knowledge","Answer")
knowledge_answer_tokenizer = AutoTokenizer.from_pretrained(knowledge_answer_model_path,local_files_only=True)
knowledge_answer_model = AutoModelForCausalLM.from_pretrained(knowledge_answer_model_path,local_files_only=True).to("cuda" if torch.cuda.is_available() else "cpu")

# 11. Open Problem 문제 생성 모델 로딩
open_problem_model_path = get_model_path("Open","Problem")
open_problem_tokenizer = AutoTokenizer.from_pretrained(open_problem_model_path,local_files_only=True)
open_problem_model = AutoModelForCausalLM.from_pretrained(open_problem_model_path,local_files_only=True).to("cuda" if torch.cuda.is_available() else "cpu")

# 12. 문제 생성 함수
def problem(prompt, model, tokenizer):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        temperature=0.9,
        top_p=0.92,
        top_k=50,
        do_sample=True,
        repetition_penalty=1.2,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text.split("문제:")[1].strip()

# 13. 정답 생성 함수
def answer(prompt, model, tokenizer):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        do_sample=False,
        repetition_penalty=1.0,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text.split("정답:")[1].strip()

# 14. FastAPI app 설정
app = FastAPI()

class ProblemRequest(BaseModel):
    topic: str
    level: str
    count: int

class ProblemResponse(BaseModel):
    response: list[str]

@app.post("/generate", response_model=ProblemResponse)
def generate_problems(req: ProblemRequest):
    global math_problem_model, math_problem_tokenizer, math_answer_model, math_answer_tokenizer
    global logic_problem_model, logic_problem_tokenizer, logic_answer_model, logic_answer_tokenizer
    global knowledge_problem_model, knowledge_problem_tokenizer, knowledge_answer_model, knowledge_answer_tokenizer
    global open_problem_model, open_problem_tokenizer

    topic = req.topic
    level = req.level
    count = req.count

    max_trials = 5
    threshold = 0.7
    problem_list = []
    trials = 0

    index = faiss.IndexFlatIP(embedding_dim)

    if topic == '수학':
      problem_model = math_problem_model
      problem_tokenizer = math_problem_tokenizer
      answer_model = math_answer_model
      answer_tokenizer = math_answer_tokenizer
    elif topic == '논리':
      problem_model = logic_problem_model
      problem_tokenizer = logic_problem_tokenizer
      answer_model = logic_answer_model
      answer_tokenizer = logic_answer_tokenizer
    elif topic == '상식':
      problem_model = knowledge_problem_model
      problem_tokenizer = knowledge_problem_tokenizer
      answer_model = knowledge_answer_model
      answer_tokenizer = knowledge_answer_tokenizer
    elif topic == '열린문제':
      problem_model = open_problem_model
      problem_tokenizer = open_problem_tokenizer
      answer_model = None
      answer_tokenizer = None
    else:
      return ProblemResponse(response=problem_list)

    with torch.no_grad():
        while len(problem_list) < count and trials < max_trials:
            needed = count - len(problem_list)
            for _ in range(needed):
                problem_prompt = f"난이도: {level} \n문제:"
                problem_result = problem(problem_prompt, problem_model, problem_tokenizer)

                embedding = embedding_model.encode(problem_result, convert_to_numpy=True, normalize_embeddings=True).reshape(1, -1)

                is_duplicate = False
                if index.ntotal > 0:
                    D, I = index.search(embedding, k=1)
                    if D[0][0] >= threshold:
                        is_duplicate = True

                if is_duplicate:
                    continue

                if topic == '열린문제':
                    answer_result = '관점에 따라 다름'
                else:
                    answer_prompt = f"문제: {problem_result} \n정답:"
                    answer_result = answer(answer_prompt, answer_model, answer_tokenizer)

                full_text = f"문제: {problem_result}\n정답: {answer_result}"

                problem_list.append(full_text)
                index.add(embedding)

            trials += 1

    problem_list = problem_list[:count]
    return ProblemResponse(response=problem_list)


# ---------------------------------------------------------------------
### Q&A code
# ---------------------------------------------------------------------

# 4. Q&A 모델 로딩
embedding_model = SentenceTransformer('jhgan/ko-sbert-sts')
model_path = get_model_path("Q&A","Response")
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path).to("cuda" if torch.cuda.is_available() else "cpu")

# 5. 유사도 임베딩 벡터 및 FAISS 인덱스 생성
qa_texts = [
    "예선대회 결과는 언제 공지되나요?",
    "문제 정답과 점수는 공개되지 않나요?",
    "채점 기준은 어떻게 되나요?",
    "대회 기간을 놓쳤습니다. 어떻게 해야 하나요?",
    "답안 작성이 되지 않습니다.",
    "대회 원서접수를 했는데 문제보기가 되지 않습니다.",
    "대회 기간 중인데 문제보기가 되지 않습니다.",
    "예선대회 진행방식은 어떻게 되나요?",
    "대회 관련 기출 문제들은 블로그 등에 올려도 되나요?",
    "접속자가 많아 접수가 어렵습니다.",
    "신청 기간 이후 추가 접수 가능한가요?",
    "학교에 다니지 않는 사람은 어떤 부문에 참가해야 하나요?",
    "해외거주 중인데 참가 가능한가요?",
    "팀명은 꼭 7자 이내로만 작성해야 하나요?",
    "대회 참가 접수를 확인하고 싶습니다.",
    "대회 원서 접수 시 팀원이 모두 가입해야 하나요?",
    "다른 부문끼리 팀 구성이 가능한가요?",
    "같은 학교 학생끼리만 팀 구성이 가능한가요?",
    "대회 참가 자격이 궁금합니다.",
    "대회 개최 일정은 어떻게 되나요?",
    "대회 관련 문의사항은 어디로 문의하나요?"
]
embeddings = embedding_model.encode(qa_texts, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False).astype('float32')
index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

# 6. 답변 생성 함수
def response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        temperature=1.3,
        top_p=0.95,
        top_k=5,
        do_sample=True,
        repetition_penalty=1.2,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).replace("@ ", "@")
    return generated_text.split("답변:")[1].strip()

class QARequest(BaseModel):
    question: str

class QAResponse(BaseModel):
    response: str

@app.post("/qa", response_model=QAResponse)
def qa_endpoint(request: QARequest):
    embedding = embedding_model.encode([request.question], normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False).astype('float32')
    D, I = index.search(embedding, 1)
    similar_question = qa_texts[I[0][0]]

    prompt = f"질문: {similar_question} \n답변:"

    with torch.no_grad():
        result = response(prompt)

    return QAResponse(response=result)
